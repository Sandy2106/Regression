{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "JdO9z4XbfDlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "- Simple Linear Regression is a method used to model the relationship between a dependent variable Y and a single independent variable X using the equation:\n",
        "  - Y=mX+c\n",
        "- Here, m is the slope and c is the intercept.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "- Linearity: The relationship between X and Y is linear.\n",
        "- Independence: Observations are independent of each other.\n",
        "- Homoscedasticity: Constant variance of residuals across all levels of X.\n",
        "- Normality of Residuals: Residuals are normally distributed.\n",
        "- No significant outliers.\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "- m is the slope of the line. It shows the amount by which Y changes for each one-unit increase in X.\n",
        "\n",
        "4. What does the intercept c represent in the equation Y = mX + c?\n",
        "- c is the value of Y when X = 0. It represents the starting point of the line on the Y-axis.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "- m= n∑XY−∑X∑Y/n∑X\n",
        "  - Where n is the number of observations.\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "- To minimize the sum of the squared differences between actual and predicted values of Y (i.e., residuals).\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "- R² represents the proportion of the variance in Y explained by X.\n",
        "\n",
        "- R² = 1 → perfect fit\n",
        "\n",
        "- R² = 0 → no linear relationship\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "It is an extension of Simple Linear Regression involving two or more independent variables:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "- Simple: One independent variable\n",
        "\n",
        "- Multiple: Two or more independent variables\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "- Linearity\n",
        "\n",
        "- Independence\n",
        "\n",
        "- Homoscedasticity\n",
        "\n",
        "- Normality of residuals\n",
        "\n",
        "- No multicollinearity between predictors\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "- Heteroscedasticity occurs when residuals have unequal variance across levels of an independent variable. It makes standard errors unreliable, affecting confidence intervals and hypothesis tests.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "- Remove or combine correlated predictors\n",
        "\n",
        "- Use Principal Component Analysis (PCA)\n",
        "\n",
        "- Apply regularization (e.g., Ridge, Lasso)\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "- One-Hot Encoding\n",
        "\n",
        "- Label Encoding\n",
        "\n",
        "- Ordinal Encoding\n",
        "\n",
        "- Binary Encoding\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "- They help model situations where the effect of one variable on Y depends on the level of another variable (e.g.,\n",
        "𝑋\n",
        "1\n",
        "×\n",
        "𝑋\n",
        "2\n",
        "X\n",
        "1\n",
        "​\n",
        " ×X\n",
        "2\n",
        "​\n",
        " ).\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "- Simple: Intercept is the value of Y when X = 0\n",
        "\n",
        "- Multiple: Intercept is Y when all predictors = 0, which may not be meaningful\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "- The slope indicates how much the dependent variable changes with a one-unit increase in the independent variable. It’s crucial for prediction and interpretation.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "- It gives the baseline value of the dependent variable when all independent variables are zero.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "- Doesn’t indicate if the model is good\n",
        "\n",
        "- Can be artificially high with many variables\n",
        "\n",
        "- Doesn’t capture overfitting\n",
        "\n",
        "- Not useful for comparing models with different numbers of predictors\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "- It means the coefficient estimate is uncertain and possibly not statistically significant.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "- It appears as a funnel or pattern in residual vs. fitted value plots. It’s important to fix it because it leads to invalid inference (e.g., biased standard errors).\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "- Some predictors may be irrelevant or overfitting the data. Adjusted R² accounts for the number of predictors, penalizing complexity.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "- To ensure equal weighting, improve model convergence, and interpret coefficients consistently, especially when regularization is used.\n",
        "\n",
        "23. What is polynomial regression?\n",
        "- A form of regression where the relationship between the independent and dependent variable is modeled as a polynomial equation.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "- Linear: Models a straight-line relationship\n",
        "\n",
        "- Polynomial: Models curved relationships using powers of the independent variable\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "- When the data shows a nonlinear pattern that a straight line cannot capture.\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X+b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "- Yes, it can include polynomial terms for each variable and their interactions.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "- Risk of overfitting\n",
        "\n",
        "- Poor extrapolation\n",
        "\n",
        "- Sensitive to outliers\n",
        "\n",
        "- Complexity increases rapidly with degree\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "- Cross-validation\n",
        "\n",
        "- Adjusted R²\n",
        "\n",
        "- AIC/BIC\n",
        "\n",
        "- Residual analysis\n",
        "\n",
        "- Visualization\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "- It helps you assess how well the model fits the data and detect overfitting or underfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZyyDJJZafFCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?"
      ],
      "metadata": {
        "id": "6EGz-PDijm2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Example: 2nd-degree polynomial regression\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X, y)\n",
        "predictions = model.predict(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "88-bncgUjqoN",
        "outputId": "1fe2788d-57e2-4ce3-bd10-d419a1591817"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a698faa20e2b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Example: 2nd-degree polynomial regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPolynomialFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    }
  ]
}